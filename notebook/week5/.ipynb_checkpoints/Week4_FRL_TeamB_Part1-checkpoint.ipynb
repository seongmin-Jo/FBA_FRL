{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cd4dd62"
   },
   "source": [
    "Finite Markov Decision Process\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "error",
     "timestamp": 1644136561053,
     "user": {
      "displayName": "‍이도영[학생](공과대학 산업경영공학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16694494629284431647"
     },
     "user_tz": -540
    },
    "id": "740457e9",
    "outputId": "736e8a0e-da20-4f0a-ead7-b9cc85d5de2d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-04ac6cd028b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     Tuple, Sequence, TypeVar, Set)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFiniteDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m from rl.markov_process import (\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rl'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %load C:\\Users\\14ZD\\RL-book\\rl\\markov_decision_process.py\n",
    "from __future__ import annotations\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('C:/Users/14ZD/RL-book')\n",
    "import numpy\n",
    "import graphviz\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import (DefaultDict, Dict, Iterable, Generic, Mapping,\n",
    "                    Tuple, Sequence, TypeVar, Set)\n",
    "\n",
    "from rl.distribution import (Categorical, Distribution, FiniteDistribution)\n",
    "\n",
    "from rl.markov_process import (\n",
    "    FiniteMarkovRewardProcess, MarkovRewardProcess, StateReward, State,\n",
    "    NonTerminal, Terminal)\n",
    "from rl.policy import FinitePolicy, Policy\n",
    "\n",
    "A = TypeVar('A')\n",
    "S = TypeVar('S')\n",
    "\n",
    "#print(\"start\")\n",
    "@dataclass(frozen=True)\n",
    "class TransitionStep(Generic[S, A]):\n",
    "    '''A single step in the simulation of an MDP, containing:\n",
    "\n",
    "    state -- the state we start from\n",
    "    action -- the action we took at that state\n",
    "    next_state -- the state we ended up in after the action\n",
    "    reward -- the instantaneous reward we got for this transition\n",
    "    '''\n",
    "    state: NonTerminal[S]\n",
    "    action: A\n",
    "    next_state: State[S]\n",
    "    reward: float\n",
    "\n",
    "    def add_return(self, γ: float, return_: float) -> ReturnStep[S, A]:\n",
    "        '''Given a γ and the return from 'next_state', this annotates the\n",
    "        transition with a return for 'state'.\n",
    "\n",
    "        '''\n",
    "        return ReturnStep(\n",
    "            self.state,\n",
    "            self.action,\n",
    "            self.next_state,\n",
    "            self.reward,\n",
    "            return_=self.reward + γ * return_\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ReturnStep(TransitionStep[S, A]):\n",
    "    '''A Transition that also contains the total *return* for its starting\n",
    "    state.\n",
    "\n",
    "    '''\n",
    "    return_: float\n",
    "\n",
    "\n",
    "class MarkovDecisionProcess(ABC, Generic[S, A]):\n",
    "    def apply_policy(self, policy: Policy[S, A]) -> MarkovRewardProcess[S]:\n",
    "        mdp = self\n",
    "\n",
    "        class RewardProcess(MarkovRewardProcess[S]):\n",
    "            def transition_reward(\n",
    "                self,\n",
    "                state: NonTerminal[S]\n",
    "            ) -> Distribution[Tuple[State[S], float]]:\n",
    "                actions: Distribution[A] = policy.act(state)\n",
    "                return actions.apply(lambda a: mdp.step(state, a))\n",
    "\n",
    "        return RewardProcess()\n",
    "\n",
    "    @abstractmethod\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[S],\n",
    "        action: A\n",
    "    ) -> Distribution[Tuple[State[S], float]]:\n",
    "        pass\n",
    "\n",
    "    def simulate_actions(\n",
    "            self,\n",
    "            start_states: Distribution[NonTerminal[S]],\n",
    "            policy: Policy[S, A]\n",
    "    ) -> Iterable[TransitionStep[S, A]]:\n",
    "        '''Simulate this MDP with the given policy, yielding the\n",
    "        sequence of (states, action, next state, reward) 4-tuples\n",
    "        encountered in the simulation trace.\n",
    "\n",
    "        '''\n",
    "        state: State[S] = start_states.sample()\n",
    "\n",
    "        while isinstance(state, NonTerminal):\n",
    "            action_distribution = policy.act(state)\n",
    "\n",
    "            action = action_distribution.sample()\n",
    "            next_distribution = self.step(state, action)\n",
    "\n",
    "            next_state, reward = next_distribution.sample()\n",
    "            yield TransitionStep(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "    def action_traces(\n",
    "            self,\n",
    "            start_states: Distribution[NonTerminal[S]],\n",
    "            policy: Policy[S, A]\n",
    "    ) -> Iterable[Iterable[TransitionStep[S, A]]]:\n",
    "        '''Yield an infinite number of traces as returned by\n",
    "        simulate_actions.\n",
    "\n",
    "        '''\n",
    "        while True:\n",
    "            yield self.simulate_actions(start_states, policy)\n",
    "\n",
    "\n",
    "ActionMapping = Mapping[A, StateReward[S]]\n",
    "StateActionMapping = Mapping[NonTerminal[S], ActionMapping[A, S]]\n",
    "\n",
    "\n",
    "\n",
    "class FiniteMarkovDecisionProcess(MarkovDecisionProcess[S, A]):\n",
    "    '''A Markov Decision Process with finite state and action spaces.\n",
    "\n",
    "    '''\n",
    "\n",
    "    mapping: StateActionMapping[S, A]\n",
    "    non_terminal_states: Sequence[NonTerminal[S]]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mapping: Mapping[S, Mapping[A, FiniteDistribution[Tuple[S, float]]]]\n",
    "    ):\n",
    "        non_terminals: Set[S] = set(mapping.keys())\n",
    "        self.mapping = {NonTerminal(s): {a: Categorical(\n",
    "            {(NonTerminal(s1) if s1 in non_terminals else Terminal(s1), r): p\n",
    "             for (s1, r), p in v.table().items()}\n",
    "        ) for a, v in d.items()} for s, d in mapping.items()}\n",
    "        self.non_terminal_states = list(self.mapping.keys())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.mapping.items():\n",
    "            display += f\"From State {s.state}:\\n\"\n",
    "            for a, d1 in d.items():\n",
    "                display += f\"  With Action {a}:\\n\"\n",
    "                for (s1, r), p in d1:\n",
    "                    opt = \"Terminal \" if isinstance(s1, Terminal) else \"\"\n",
    "                    display += f\"    To [{opt}State {s1.state} and \"\\\n",
    "                        + f\"Reward {r:.3f}] with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    def step(self, state: NonTerminal[S], action: A) -> StateReward[S]:\n",
    "        action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "        return action_map[action]\n",
    "\n",
    "    def apply_finite_policy(self, policy: FinitePolicy[S, A])\\\n",
    "            -> FiniteMarkovRewardProcess[S]:\n",
    "\n",
    "        transition_mapping: Dict[S, FiniteDistribution[Tuple[S, float]]] = {}\n",
    "\n",
    "        for state in self.mapping:\n",
    "            action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "            outcomes: DefaultDict[Tuple[S, float], float]\\\n",
    "                = defaultdict(float)\n",
    "            actions = policy.act(state)\n",
    "            for action, p_action in actions:\n",
    "                for (s1, r), p in action_map[action].table().items():\n",
    "                    outcomes[(s1.state, r)] += p_action * p\n",
    "\n",
    "            transition_mapping[state.state] = Categorical(outcomes)\n",
    "\n",
    "        return FiniteMarkovRewardProcess(transition_mapping)\n",
    "\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        '''All the actions allowed for the given state.\n",
    "\n",
    "        This will be empty for terminal states.\n",
    "\n",
    "        '''\n",
    "        return self.mapping[state].keys()\n",
    "\n",
    "#print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "error",
     "timestamp": 1644138822580,
     "user": {
      "displayName": "‍이도영[학생](공과대학 산업경영공학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16694494629284431647"
     },
     "user_tz": -540
    },
    "id": "5f703c35",
    "outputId": "d1c827cf-5ce1-4b3d-a263-f9c3906f56cf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9b42714dd965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFiniteMarkovDecisionProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkovDecisionProcess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     '''A Markov Decision Process with finite state and action spaces.\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MarkovDecisionProcess' is not defined"
     ]
    }
   ],
   "source": [
    "class FiniteMarkovDecisionProcess(MarkovDecisionProcess[S, A]):\n",
    "    '''A Markov Decision Process with finite state and action spaces.\n",
    "\n",
    "    '''\n",
    "\n",
    "    mapping: StateActionMapping[S, A]\n",
    "    non_terminal_states: Sequence[NonTerminal[S]]\n",
    "    \n",
    "    #input: mapping\n",
    "    #Maps Non-terminal state to an action map\n",
    "    #Terminal state to None\n",
    "    #Maps each action to a finite probability distributino of pairs (next state, reward)\n",
    "    def __init__(\n",
    "        self,\n",
    "        mapping: Mapping[S, Mapping[A, FiniteDistribution[Tuple[S, float]]]]\n",
    "    ):\n",
    "        non_terminals: Set[S] = set(mapping.keys())\n",
    "        self.mapping = {NonTerminal(s): {a: Categorical(\n",
    "            {(NonTerminal(s1) if s1 in non_terminals else Terminal(s1), r): p\n",
    "             for (s1, r), p in v.table().items()}\n",
    "        ) for a, v in d.items()} for s, d in mapping.items()}\n",
    "        self.non_terminal_states = list(self.mapping.keys())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.mapping.items():\n",
    "            display += f\"From State {s.state}:\\n\"\n",
    "            for a, d1 in d.items():\n",
    "                display += f\"  With Action {a}:\\n\"\n",
    "                for (s1, r), p in d1:\n",
    "                    opt = \"Terminal \" if isinstance(s1, Terminal) else \"\"\n",
    "                    display += f\"    To [{opt}State {s1.state} and \"\\\n",
    "                        + f\"Reward {r:.3f}] with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "    \n",
    "    \n",
    "    #step() returns finite probability distribution of (next state, reward)\n",
    "    #if self.mapping[state] == terminal, returns None\n",
    "    def step(self, state: NonTerminal[S], action: A) -> StateReward[S]:\n",
    "        action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "        return action_map[action]\n",
    "    \n",
    "    \n",
    "    #Input: FinitePolicy[S,A] which maps non-terminal state to probability distribution over a finite set of actions\n",
    "    #returns a FiniteMRP\n",
    "    def apply_finite_policy(self, policy: FinitePolicy[S, A])\\\n",
    "            -> FiniteMarkovRewardProcess[S]:\n",
    "\n",
    "        transition_mapping: Dict[S, FiniteDistribution[Tuple[S, float]]] = {}\n",
    "\n",
    "        for state in self.mapping:\n",
    "            action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "            outcomes: DefaultDict[Tuple[S, float], float]\\\n",
    "                = defaultdict(float)\n",
    "            actions = policy.act(state)\n",
    "            for action, p_action in actions:\n",
    "                for (s1, r), p in action_map[action].table().items():\n",
    "                    outcomes[(s1.state, r)] += p_action * p\n",
    "\n",
    "            transition_mapping[state.state] = Categorical(outcomes)\n",
    "\n",
    "        return FiniteMarkovRewardProcess(transition_mapping)\n",
    "    \n",
    "    \n",
    "    #Iterable한 actions들을 mapping[state].keys()로 return\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        '''All the actions allowed for the given state.\n",
    "\n",
    "        This will be empty for terminal states.\n",
    "\n",
    "        '''\n",
    "        return self.mapping[state].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c34daf0"
   },
   "source": [
    "MDP Value Function for a fixed policy\n",
    "![image.png](attachment:image.png)\n",
    "![image-2.png](attachment:image-2.png)\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "V(s):\n",
    "![image-4.png](attachment:image-4.png)\n",
    "\n",
    "Action-value function which maps (state, action) pair to expected return\n",
    "![image-5.png](attachment:image-5.png)\n",
    "\n",
    "State-value function = weighted average of action-value funciton \n",
    "![image-6.png](attachment:image-6.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRz3n0dDTKrr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Week4_FRL_TeamB_Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
