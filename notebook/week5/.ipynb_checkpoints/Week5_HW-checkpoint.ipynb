{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lunar\\PycharmProjects\\RL-book\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Iterator, Dict, Mapping, Sequence, Optional, Generic, TypeVar\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import os\n",
    "\n",
    "def checkPath():\n",
    "    for i in range(5):\n",
    "        path = os.getcwd()\n",
    "        \n",
    "        if path[-7:] == 'RL-book':\n",
    "            break\n",
    "        else:\n",
    "            os.chdir('../')      \n",
    "    print(os.getcwd())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    checkPath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](image-9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](image-10.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#도영님 파트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{R}$ : transition probability function of the MRP / state-reward transition probability function of the MDP.\n",
    "\n",
    "$MRP: \\mathcal{P}_{R}\\left(s, r, s^{\\prime}\\right)=\\mathbb{P}\\left[\\left(R_{t+1}=r, S_{t+1}=s^{\\prime}\\right) \\mid S_{t}=s\\right] $\n",
    "\n",
    "\n",
    "$MDP: \\mathcal{P}_{R}\\left(s, a, r, s^{\\prime}\\right)=\\mathbb{P}\\left[\\left(R_{t+1}=r, S_{t+1}=s^{\\prime}\\right) \\mid\\left(S_{t}=s, A_{t}=a\\right)\\right]$\n",
    "\n",
    "$P$ : transition probability function of the Markov Process implicit in the MRP / state transition probability function of the MDP.\n",
    "\n",
    "$MRP: \\mathcal{P}\\left(s, s^{\\prime}\\right)=\\sum_{r \\in \\mathcal{D}} \\mathcal{P}_{R}\\left(s, r, s^{\\prime}\\right)$\n",
    "\n",
    "$MDP: \\mathcal{P}\\left(s, a, s^{\\prime}\\right)=\\sum_{r \\in \\mathcal{D}} \\mathcal{P}_{R}\\left(s, a, r, s^{\\prime}\\right)$\n",
    "\n",
    "$R_{T}$ : reward transition function of the MRP / reward transition function of the MDP.\n",
    "\n",
    "$MRP: \\mathcal{R}_{T}\\left(s, s^{\\prime}\\right)=\\mathbb{E}\\left[R_{t+1} \\mid S_{t+1}=s^{\\prime}, S_{t}=s\\right]$\n",
    "\n",
    "$MDP: \\mathcal{R}_{T}\\left(s, a, s^{\\prime}\\right)=\\mathbb{E}\\left[R_{t+1} \\mid\\left(S_{t+1}=s^{\\prime}, S_{t}=s, A_{t}=a\\right)\\right]$\n",
    "\n",
    "$R$ : reward function of the MRP / reward function of the MDP\n",
    "\n",
    "$MRP: \\mathcal{R}(s)=\\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s\\right]$\n",
    "\n",
    "$MDP: \\mathcal{R}(s, a)=\\mathbb{E}\\left[R_{t+1} \\mid\\left(S_{t}=s, A_{t}=a\\right)\\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{P}_{R}^{\\pi}\\left(s, r, s^{\\prime}\\right)=\\sum_{a \\in \\mathcal{A}} \\pi(s, a) \\cdot \\mathcal{P}_{R}\\left(s, a, r, s^{\\prime}\\right)\n",
    "$$\n",
    "Likewise,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{P}^{\\pi}\\left(s, s^{\\prime}\\right) &=\\sum_{a \\in \\mathcal{A}} \\pi(s, a) \\cdot \\mathcal{P}\\left(s, a, s^{\\prime}\\right) \\\\\n",
    "\\mathcal{R}_{T}^{\\pi}\\left(s, s^{\\prime}\\right) &=\\sum_{a \\in \\mathcal{A}} \\pi(s, a) \\cdot \\mathcal{R}_{T}\\left(s, a, s^{\\prime}\\right) \\\\\n",
    "\\mathcal{R}^{\\pi}(s) &=\\sum_{a \\in \\mathcal{A}} \\pi(s, a) \\cdot \\mathcal{R}(s, a)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.distribution import Distribution\n",
    "from rl.markov_process import MarkovRewardProcess\n",
    "from typing import Iterable\n",
    "\n",
    "A = TypeVar('A')\n",
    "S = TypeVar('S')\n",
    "\n",
    "class Policy(ABC, Generic[S, A]):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def act(self, state: S) -> Optional[Distribution[A]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransitionStep(Generic[S, A]):\n",
    "    state: S\n",
    "    action: A\n",
    "    next_state: S\n",
    "    reward: float\n",
    "        \n",
    "class MarkovDecisionProcess(ABC, Generic[S, A]):\n",
    "    \n",
    "    #  input: state: S, and produces as output an Iterable[A] to represent the set of actions allowable for the input state\n",
    "    @abstractmethod\n",
    "    def actions(self, state: S) -> Iterable[A]:\n",
    "        pass\n",
    "\n",
    "    # It is means to specify the distribution of pairs of next state and reward, given a state and action.\n",
    "    @abstractmethod\n",
    "    def step(self, state: S, action: A) -> Optional[Distribution[Tuple[S, float]]]:\n",
    "        pass\n",
    "    \n",
    "    # input: a policy: Policy[S, A] and returns MRP\n",
    "    def apply_policy(self, policy: Policy[S, A]) -> MarkovRewardProcess[S]:\n",
    "        mdp = self\n",
    "        \n",
    "        class RewardProcess(MarkovRewardProcess[S]):\n",
    "            def transition_reward(self, state: S) -> \\\n",
    "            Optional[Distribution[Tuple[S, float]]]:\n",
    "                actions: Optional[Distribution[A]] = policy.act(state)\n",
    "                if actions is None:\n",
    "                    return None\n",
    "                return actions.apply(lambda a: mdp.step(state, a))\n",
    "        \n",
    "        return RewardProcess()\n",
    "    \n",
    "    # input state: S and returns a bool signifying whether state is a terminal state or not.\n",
    "    # Since the actions method can returns the Iterbale type of allowable actions, \n",
    "    # the only way to check is by checking that next method triggers error.\n",
    "    def is_terminal(self, state: S) -> bool:\n",
    "        try:\n",
    "            next(iter(self.actions(state)))\n",
    "            return False\n",
    "        except StopIteration:\n",
    "            return True\n",
    "        \n",
    "    def simulate_actions(self, start_states: Distribution[S], policy: Policy[S, A]) -> \\\n",
    "    Iterable[TransitionStep[S, A]]:\n",
    "        state: S = start_states.sample()\n",
    "        reward: float = 0\n",
    "            \n",
    "        while True:\n",
    "            # action distribution from given policy and state\n",
    "            action_distribution = policy.act(state)\n",
    "            if action_distribution is None:\n",
    "                return\n",
    "            \n",
    "            # the distribution of paris of next state and reward, given a state and action.\n",
    "            action = action_distribution.sample()\n",
    "            next_distribution = self.step(state, action)\n",
    "            if next_distribution is None:\n",
    "                return\n",
    "            \n",
    "            # after sampling, yield TransitionStep\n",
    "            next_state, reward = next_distribution.sample()\n",
    "            yield TransitionStep(state, action, next_state, reward)\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic Policy yields 1.88% of Out-Of-Stock days\n",
      "Stochastic Policy yields 2.94% of Out-Of-Stock days\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Iterator\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import random\n",
    "\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "from rl.markov_process import MarkovRewardProcess, NonTerminal, State\n",
    "from rl.policy import Policy, DeterministicPolicy\n",
    "from rl.distribution import Constant, SampledDistribution\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SimpleInventoryMDPNoCap(MarkovDecisionProcess[InventoryState, int]):\n",
    "    # hyperparameters\n",
    "    poisson_lambda: float\n",
    "    holding_cost: float\n",
    "    stockout_cost: float\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[InventoryState],\n",
    "        order: int\n",
    "    ) -> SampledDistribution[Tuple[State[InventoryState], float]]:\n",
    "\n",
    "        def sample_next_state_reward(\n",
    "            state=state,\n",
    "            order=order\n",
    "        ) -> Tuple[State[InventoryState], float]:\n",
    "            demand_sample: int = np.random.poisson(self.poisson_lambda)\n",
    "            ip: int = state.state.inventory_position()\n",
    "            next_state: InventoryState = InventoryState(\n",
    "                max(ip - demand_sample, 0),\n",
    "                order\n",
    "            )\n",
    "            reward: float = - self.holding_cost * state.state.on_hand\\\n",
    "                - self.stockout_cost * max(demand_sample - ip, 0)\n",
    "            return NonTerminal(next_state), reward\n",
    "\n",
    "        return SampledDistribution(sample_next_state_reward)\n",
    "\n",
    "    def actions(self, state: NonTerminal[InventoryState]) -> Iterator[int]:\n",
    "        return itertools.count(start=0, step=1)\n",
    "\n",
    "    def fraction_of_days_oos(\n",
    "        self,\n",
    "        policy: Policy[InventoryState, int],\n",
    "        time_steps: int,\n",
    "        num_traces: int\n",
    "    ) -> float:\n",
    "        impl_mrp: MarkovRewardProcess[InventoryState] =\\\n",
    "            self.apply_policy(policy)\n",
    "        count: int = 0\n",
    "        high_fractile: int = int(poisson(self.poisson_lambda).ppf(0.98))\n",
    "        start: InventoryState = random.choice(\n",
    "            [InventoryState(i, 0) for i in range(high_fractile + 1)])\n",
    "\n",
    "        for _ in range(num_traces):\n",
    "            steps = itertools.islice(\n",
    "                impl_mrp.simulate_reward(Constant(NonTerminal(start))),\n",
    "                time_steps\n",
    "            )\n",
    "            for step in steps:\n",
    "                if step.reward < -self.holding_cost * step.state.state.on_hand:\n",
    "                    count += 1\n",
    "\n",
    "        return float(count) / (time_steps * num_traces)\n",
    "\n",
    "\n",
    "class SimpleInventoryDeterministicPolicy(\n",
    "        DeterministicPolicy[InventoryState, int]\n",
    "):\n",
    "    def __init__(self, reorder_point: int):\n",
    "        self.reorder_point: int = reorder_point\n",
    "\n",
    "        def action_for(s: InventoryState) -> int:\n",
    "            return max(self.reorder_point - s.inventory_position(), 0)\n",
    "\n",
    "        super().__init__(action_for)\n",
    "\n",
    "\n",
    "class SimpleInventoryStochasticPolicy(Policy[InventoryState, int]):\n",
    "    def __init__(self, reorder_point_poisson_mean: float):\n",
    "        self.reorder_point_poisson_mean: float = reorder_point_poisson_mean\n",
    "\n",
    "    def act(self, state: NonTerminal[InventoryState]) -> \\\n",
    "            SampledDistribution[int]:\n",
    "        def action_func(state=state) -> int:\n",
    "            reorder_point_sample: int = \\\n",
    "                np.random.poisson(self.reorder_point_poisson_mean)\n",
    "            return max(\n",
    "                reorder_point_sample - state.state.inventory_position(),\n",
    "                0\n",
    "            )\n",
    "        return SampledDistribution(action_func)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    user_poisson_lambda = 2.0\n",
    "    user_holding_cost = 1.0\n",
    "    user_stockout_cost = 10.0\n",
    "# hyperparameters\n",
    "    user_reorder_point = 8\n",
    "    user_reorder_point_poisson_mean = 8.0\n",
    "\n",
    "    user_time_steps = 1000\n",
    "    user_num_traces = 1000\n",
    "\n",
    "    si_mdp_nocap = SimpleInventoryMDPNoCap(poisson_lambda=user_poisson_lambda,\n",
    "                                           holding_cost=user_holding_cost,\n",
    "                                           stockout_cost=user_stockout_cost)\n",
    "\n",
    "    si_dp = SimpleInventoryDeterministicPolicy(\n",
    "        reorder_point=user_reorder_point\n",
    "    )\n",
    "\n",
    "    oos_frac_dp = si_mdp_nocap.fraction_of_days_oos(policy=si_dp,\n",
    "                                                    time_steps=user_time_steps,\n",
    "                                                    num_traces=user_num_traces)\n",
    "    print(\n",
    "        f\"Deterministic Policy yields {oos_frac_dp * 100:.2f}%\"\n",
    "        + \" of Out-Of-Stock days\"\n",
    "    )\n",
    "\n",
    "    si_sp = SimpleInventoryStochasticPolicy(\n",
    "        reorder_point_poisson_mean=user_reorder_point_poisson_mean)\n",
    "\n",
    "    oos_frac_sp = si_mdp_nocap.fraction_of_days_oos(policy=si_sp,\n",
    "                                                    time_steps=user_time_steps,\n",
    "                                                    num_traces=user_num_traces)\n",
    "    print(\n",
    "        f\"Stochastic Policy yields {oos_frac_sp * 100:.2f}%\"\n",
    "        + \" of Out-Of-Stock days\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finite Markov Decision Process**\n",
    "![nn](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load C:\\Users\\14ZD\\RL-book\\rl\\markov_decision_process.py\n",
    "from __future__ import annotations\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('C:/Users/14ZD/RL-book')\n",
    "import numpy\n",
    "import graphviz\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import (DefaultDict, Dict, Iterable, Generic, Mapping,\n",
    "                    Tuple, Sequence, TypeVar, Set)\n",
    "\n",
    "from rl.distribution import (Categorical, Distribution, FiniteDistribution)\n",
    "\n",
    "from rl.markov_process import (\n",
    "    FiniteMarkovRewardProcess, MarkovRewardProcess, StateReward, State,\n",
    "    NonTerminal, Terminal)\n",
    "from rl.policy import FinitePolicy, Policy\n",
    "\n",
    "A = TypeVar('A')\n",
    "S = TypeVar('S')\n",
    "\n",
    "#print(\"start\")\n",
    "@dataclass(frozen=True)\n",
    "class TransitionStep(Generic[S, A]):\n",
    "    '''A single step in the simulation of an MDP, containing:\n",
    "\n",
    "    state -- the state we start from\n",
    "    action -- the action we took at that state\n",
    "    next_state -- the state we ended up in after the action\n",
    "    reward -- the instantaneous reward we got for this transition\n",
    "    '''\n",
    "    state: NonTerminal[S]\n",
    "    action: A\n",
    "    next_state: State[S]\n",
    "    reward: float\n",
    "\n",
    "    def add_return(self, γ: float, return_: float) -> ReturnStep[S, A]:\n",
    "        '''Given a γ and the return from 'next_state', this annotates the\n",
    "        transition with a return for 'state'.\n",
    "\n",
    "        '''\n",
    "        return ReturnStep(\n",
    "            self.state,\n",
    "            self.action,\n",
    "            self.next_state,\n",
    "            self.reward,\n",
    "            return_=self.reward + γ * return_\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ReturnStep(TransitionStep[S, A]):\n",
    "    '''A Transition that also contains the total *return* for its starting\n",
    "    state.\n",
    "\n",
    "    '''\n",
    "    return_: float\n",
    "\n",
    "\n",
    "class MarkovDecisionProcess(ABC, Generic[S, A]):\n",
    "    def apply_policy(self, policy: Policy[S, A]) -> MarkovRewardProcess[S]:\n",
    "        mdp = self\n",
    "\n",
    "        class RewardProcess(MarkovRewardProcess[S]):\n",
    "            def transition_reward(\n",
    "                self,\n",
    "                state: NonTerminal[S]\n",
    "            ) -> Distribution[Tuple[State[S], float]]:\n",
    "                actions: Distribution[A] = policy.act(state)\n",
    "                return actions.apply(lambda a: mdp.step(state, a))\n",
    "\n",
    "        return RewardProcess()\n",
    "\n",
    "    @abstractmethod\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[S],\n",
    "        action: A\n",
    "    ) -> Distribution[Tuple[State[S], float]]:\n",
    "        pass\n",
    "\n",
    "    def simulate_actions(\n",
    "            self,\n",
    "            start_states: Distribution[NonTerminal[S]],\n",
    "            policy: Policy[S, A]\n",
    "    ) -> Iterable[TransitionStep[S, A]]:\n",
    "        '''Simulate this MDP with the given policy, yielding the\n",
    "        sequence of (states, action, next state, reward) 4-tuples\n",
    "        encountered in the simulation trace.\n",
    "\n",
    "        '''\n",
    "        state: State[S] = start_states.sample()\n",
    "\n",
    "        while isinstance(state, NonTerminal):\n",
    "            action_distribution = policy.act(state)\n",
    "\n",
    "            action = action_distribution.sample()\n",
    "            next_distribution = self.step(state, action)\n",
    "\n",
    "            next_state, reward = next_distribution.sample()\n",
    "            yield TransitionStep(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "    def action_traces(\n",
    "            self,\n",
    "            start_states: Distribution[NonTerminal[S]],\n",
    "            policy: Policy[S, A]\n",
    "    ) -> Iterable[Iterable[TransitionStep[S, A]]]:\n",
    "        '''Yield an infinite number of traces as returned by\n",
    "        simulate_actions.\n",
    "\n",
    "        '''\n",
    "        while True:\n",
    "            yield self.simulate_actions(start_states, policy)\n",
    "\n",
    "\n",
    "ActionMapping = Mapping[A, StateReward[S]]\n",
    "StateActionMapping = Mapping[NonTerminal[S], ActionMapping[A, S]]\n",
    "\n",
    "\n",
    "\n",
    "class FiniteMarkovDecisionProcess(MarkovDecisionProcess[S, A]):\n",
    "    '''A Markov Decision Process with finite state and action spaces.\n",
    "\n",
    "    '''\n",
    "\n",
    "    mapping: StateActionMapping[S, A]\n",
    "    non_terminal_states: Sequence[NonTerminal[S]]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mapping: Mapping[S, Mapping[A, FiniteDistribution[Tuple[S, float]]]]\n",
    "    ):\n",
    "        non_terminals: Set[S] = set(mapping.keys())\n",
    "        self.mapping = {NonTerminal(s): {a: Categorical(\n",
    "            {(NonTerminal(s1) if s1 in non_terminals else Terminal(s1), r): p\n",
    "             for (s1, r), p in v.table().items()}\n",
    "        ) for a, v in d.items()} for s, d in mapping.items()}\n",
    "        self.non_terminal_states = list(self.mapping.keys())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.mapping.items():\n",
    "            display += f\"From State {s.state}:\\n\"\n",
    "            for a, d1 in d.items():\n",
    "                display += f\"  With Action {a}:\\n\"\n",
    "                for (s1, r), p in d1:\n",
    "                    opt = \"Terminal \" if isinstance(s1, Terminal) else \"\"\n",
    "                    display += f\"    To [{opt}State {s1.state} and \"\\\n",
    "                        + f\"Reward {r:.3f}] with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    def step(self, state: NonTerminal[S], action: A) -> StateReward[S]:\n",
    "        action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "        return action_map[action]\n",
    "\n",
    "    def apply_finite_policy(self, policy: FinitePolicy[S, A])\\\n",
    "            -> FiniteMarkovRewardProcess[S]:\n",
    "\n",
    "        transition_mapping: Dict[S, FiniteDistribution[Tuple[S, float]]] = {}\n",
    "\n",
    "        for state in self.mapping:\n",
    "            action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "            outcomes: DefaultDict[Tuple[S, float], float]\\\n",
    "                = defaultdict(float)\n",
    "            actions = policy.act(state)\n",
    "            for action, p_action in actions:\n",
    "                for (s1, r), p in action_map[action].table().items():\n",
    "                    outcomes[(s1.state, r)] += p_action * p\n",
    "\n",
    "            transition_mapping[state.state] = Categorical(outcomes)\n",
    "\n",
    "        return FiniteMarkovRewardProcess(transition_mapping)\n",
    "\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        '''All the actions allowed for the given state.\n",
    "\n",
    "        This will be empty for terminal states.\n",
    "\n",
    "        '''\n",
    "        return self.mapping[state].keys()\n",
    "\n",
    "#print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiniteMarkovDecisionProcess(MarkovDecisionProcess[S, A]):\n",
    "    '''A Markov Decision Process with finite state and action spaces.\n",
    "\n",
    "    '''\n",
    "\n",
    "    mapping: StateActionMapping[S, A]\n",
    "    non_terminal_states: Sequence[NonTerminal[S]]\n",
    "    \n",
    "    #input: mapping\n",
    "    #Maps Non-terminal state to an action map\n",
    "    #Terminal state to None\n",
    "    #Maps each action to a finite probability distributino of pairs (next state, reward)\n",
    "    def __init__(\n",
    "        self,\n",
    "        mapping: Mapping[S, Mapping[A, FiniteDistribution[Tuple[S, float]]]]\n",
    "    ):\n",
    "        non_terminals: Set[S] = set(mapping.keys())\n",
    "        self.mapping = {NonTerminal(s): {a: Categorical(\n",
    "            {(NonTerminal(s1) if s1 in non_terminals else Terminal(s1), r): p\n",
    "             for (s1, r), p in v.table().items()}\n",
    "        ) for a, v in d.items()} for s, d in mapping.items()}\n",
    "        self.non_terminal_states = list(self.mapping.keys())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.mapping.items():\n",
    "            display += f\"From State {s.state}:\\n\"\n",
    "            for a, d1 in d.items():\n",
    "                display += f\"  With Action {a}:\\n\"\n",
    "                for (s1, r), p in d1:\n",
    "                    opt = \"Terminal \" if isinstance(s1, Terminal) else \"\"\n",
    "                    display += f\"    To [{opt}State {s1.state} and \"\\\n",
    "                        + f\"Reward {r:.3f}] with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "    \n",
    "    \n",
    "    #step() returns finite probability distribution of (next state, reward)\n",
    "    #if self.mapping[state] == terminal, returns None\n",
    "    def step(self, state: NonTerminal[S], action: A) -> StateReward[S]:\n",
    "        action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "        return action_map[action]\n",
    "    \n",
    "    \n",
    "    #Input: FinitePolicy[S,A] which maps non-terminal state to probability distribution over a finite set of actions\n",
    "    #returns a FiniteMRP\n",
    "    def apply_finite_policy(self, policy: FinitePolicy[S, A])\\\n",
    "            -> FiniteMarkovRewardProcess[S]:\n",
    "\n",
    "        transition_mapping: Dict[S, FiniteDistribution[Tuple[S, float]]] = {}\n",
    "\n",
    "        for state in self.mapping:\n",
    "            action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "            outcomes: DefaultDict[Tuple[S, float], float]\\\n",
    "                = defaultdict(float)\n",
    "            actions = policy.act(state)\n",
    "            for action, p_action in actions:\n",
    "                for (s1, r), p in action_map[action].table().items():\n",
    "                    outcomes[(s1.state, r)] += p_action * p\n",
    "\n",
    "            transition_mapping[state.state] = Categorical(outcomes)\n",
    "\n",
    "        return FiniteMarkovRewardProcess(transition_mapping)\n",
    "    \n",
    "    \n",
    "    #Iterable한 actions들을 mapping[state].keys()로 return\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        '''All the actions allowed for the given state.\n",
    "\n",
    "        This will be empty for terminal states.\n",
    "\n",
    "        '''\n",
    "        return self.mapping[state].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{N} \\rightarrow(\\mathcal{A} \\rightarrow(\\mathcal{S} \\times \\mathcal{D} \\rightarrow[0,1]))$\n",
    "\n",
    "$\\pi : \\mathcal{N} \\rightarrow(\\mathcal{A} \\rightarrow[0,1])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state x reward\n",
    "StateReward = FiniteDistribution[Tuple[S, float]]\n",
    "# action X (state x reward)\n",
    "ActionMapping = Mapping[A, StateReward[S]]\n",
    "# state X (action X (state X reward))\n",
    "StateActionMapping = Mapping[S, Optional[ActionMapping[A, S]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.distribution import SampledDistribution\n",
    "from collections import defaultdict\n",
    "from rl.distribution import Categorical\n",
    "\n",
    "class FiniteMarkovDecisionProcess(MarkovDecisionProcess[S, A]):\n",
    "    \n",
    "    # state X (action X (state X reward))\n",
    "    mapping: StateActionMapping[S, A]\n",
    "    non_terminal_states: Sequence[S]\n",
    "    \n",
    "    def __init__(self, mapping: StateActionMapping[S, A]):\n",
    "        self.mapping = mapping\n",
    "        self.non_terminal_states = [s for s, v in mapping.items()]\n",
    "        \n",
    "    # Create display string using nested for statements\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.mapping.items():\n",
    "            if d is None:\n",
    "                display += f\"{s} is Terminal State\\n\"\n",
    "            else:\n",
    "                display += f\"From State {s}:\\n\"\n",
    "                for a, d1 in d.items():\n",
    "                    display += f\"  With Action {a}:\\n\"\n",
    "                    for (s1, r), p in d1.table():\n",
    "                        display += f\"    To [State {s1} and \"\\\n",
    "                            + f\"Reward {r:.3f} with Probability {p:.3f}\\n\"\n",
    "                        \n",
    "        return display\n",
    "    \n",
    "    def apply_finite_policy(self, policy: FinitePolicy[S, A])\\\n",
    "        -> FiniteMarkovRewardProcess[S]:\n",
    "        \n",
    "        # Create dict: state X (state X reward)\n",
    "        transition_mapping: Dict[S, Optional[StateReward[S]]] = {}\n",
    "            \n",
    "        for state in self.mapping:\n",
    "            # Create ActionMapping: action X (state X reward)\n",
    "            action_map: Optional[ActionMapping[A, S]] = self.mapping[state]\n",
    "            if action_map is None:\n",
    "                transition_mapping[state] = None\n",
    "                \n",
    "            # Create outcomes: Tuple(state X reward), prob\n",
    "            else:\n",
    "                outcomes: DefaultDict[Tuple[S, float], float]\\\n",
    "                    = defaultdict(float)\n",
    "                # actions: action X prob\n",
    "                actions = policy.act(state)\n",
    "                if actions is not None:\n",
    "                    for action, p_action in actions:\n",
    "                        for outcome, p_state_reward in action_map[action]:\n",
    "                            # state X (sum of all reward*action_prob)\n",
    "                            outcomes[outcome] += p_action * p_state_reward\n",
    "                 \n",
    "                # state X (next_state X reward)\n",
    "                transition_mapping[state] = Categorical(outcomes)\n",
    "                \n",
    "        return FiniteMarkovRewardProcess(transition_mapping)    \n",
    "    \n",
    "    # return probability\n",
    "    def step(self, sate: S, action: A) -> Optional[StateReward]:\n",
    "        action_map: Optional[ActionMapping[A, S]] = self.mapping[state]\n",
    "        if action_map is None:\n",
    "            return None\n",
    "        return action_map[action]\n",
    "    \n",
    "    # return actions that are possible in the current state\n",
    "    def actions(self, state: S) -> Iterable[A]:\n",
    "        actions = self.mapping[state]\n",
    "        return iter([]) if actions is None else actions.keys()\n",
    "    \n",
    "    # return ActionMapping\n",
    "    def action_mapping(self, state: S) -> Optional[ActionMapping[A, S]]:\n",
    "        return self.mapping[state]\n",
    "    \n",
    "    # return state key\n",
    "    def states(self) -> Iterable[S]:\n",
    "        return self.mapping.keys()\n",
    "    \n",
    "class FinitePolicy(Policy[S, A]):\n",
    "    # state X distribution\n",
    "    policy_map: Mapping[S, Optional[FiniteDistribution[A]]]\n",
    "        \n",
    "    def __init__(self, policy_map: Mapping[S, Optional[FiniteDistribution[A]]]):\n",
    "        self.policy_map = policy_map\n",
    "    \n",
    "    # create display string using nested for statement\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.policy_map.items():\n",
    "            if d is None:\n",
    "                display += f\"{s} is a Terminal State\\n\"\n",
    "            else:\n",
    "                display += f\"For State {s}:\\n\"\n",
    "                for a, p in d:\n",
    "                    display += f\" Do Action {a} with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    # return action distribution\n",
    "    def act(self, state: S) -> Optional[FiniteDistribution[A]]:\n",
    "        return self.policy_map[state]\n",
    "    \n",
    "    # return state keys\n",
    "    def states(self) -> Iterable[S]:\n",
    "        return self.policy_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "from rl.distribution import Categorical\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "        \n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "    \n",
    "    \n",
    "InvOrderMapping = StateActionMapping[InventoryState, int]\n",
    "\n",
    "class SimpleInventoryMDPCap(FiniteMarkovDecisionProcess[InventoryState, int]):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "            \n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "        \n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState, float]]]] = {}\n",
    "        \n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity +1 - alpha):\n",
    "                state: InventoryState = InventoryState(alpha, beta)\n",
    "                ip: int = state. inventory_position()\n",
    "                base_reward: float = -self.holding_cost * alpha\n",
    "                d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "                    \n",
    "                for order in range(self.capacity - ip + 1):\n",
    "                    sr_probs_dict: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                        {(InventoryState(ip - i, order), base_reward):\n",
    "                        self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "                        \n",
    "                    probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                    reward: float = base_reward - self.stockout_cost *\\\n",
    "                        (probability * (self.poisson_lambda - ip) +\n",
    "                        ip * self.poisson_distr.pmf(ip))\n",
    "                    sr_probs_dict[(InventoryState(0, order), reward)] = \\\n",
    "                        probability\n",
    "                    d1[order] = Categorical(sr_probs_dict)\n",
    "                    \n",
    "            d[state] = d1\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For State InventoryState(on_hand=0, on_order=0):\n",
      " Do Action 2 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      " Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      " Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      " Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      " Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      " Do Action 0 with Probability 1.000\n",
      "\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [Terminal State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [Terminal State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [Terminal State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [Terminal State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [Terminal State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [Terminal State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.distribution import Constant\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    "    )\n",
    "    \n",
    "fdp: FinitePolicy[InventoryState, int] = FinitePolicy(\n",
    "    {InventoryState(alpha, beta):\n",
    "    Constant(user_capacity - (alpha + beta)) for alpha in\n",
    "    range(user_capacity + 1) for beta in range(user_capacity + 1 - alpha)}\n",
    ")\n",
    "    \n",
    "implied_mrp: FiniteMarkovRewardProcess[InventoryState] =\\\n",
    "    si_mdp.apply_finite_policy(fdp)\n",
    "    \n",
    "print(fdp)\n",
    "print(implied_mrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP Value Function for a fixed policy\n",
    "![nn](image-2.png)\n",
    "![nn](image-3.png)\n",
    "![nn](image-4.png)\n",
    "\n",
    "V(s):\n",
    "![nn](image-5.png)\n",
    "\n",
    "Action-value function which maps (state, action) pair to expected return\n",
    "![nn](image-6.png)\n",
    "\n",
    "State-value function = weighted average of action-value funciton \n",
    "![nn](image-7.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^{*}(s)=\\max _{a \\in \\mathcal{A}} Q^{*}(s, a)$ for all $s \\in \\mathcal{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q^{*}(s, a)=\\mathcal{R}(s, a)+\\gamma \\cdot \\sum_{s^{\\prime} \\in \\mathcal{N}} \\mathcal{P}\\left(s, a, s^{\\prime}\\right) \\cdot V^{*}\\left(s^{\\prime}\\right) \\text { for all } s \\in \\mathcal{N}, a \\in \\mathcal{A}\n",
    "$$\n",
    "Substituting for $Q^{*}(s, a)$ from Equation (2.6) in Equation (2.5) gives:\n",
    "$$\n",
    "V^{*}(s)=\\max _{a \\in \\mathcal{A}}\\left\\{\\mathcal{R}(s, a)+\\gamma \\cdot \\sum_{s^{\\prime} \\in \\mathcal{N}} \\mathcal{P}\\left(s, a, s^{\\prime}\\right) \\cdot V^{*}\\left(s^{\\prime}\\right)\\right\\} \\text { for all } s \\in \\mathcal{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDP State-Value Function Bellman Optimality Equation**\n",
    "\n",
    "$V^{*}(s)=\\max _{a \\in \\mathcal{A}}\\left\\{\\mathcal{R}(s, a)+\\gamma \\cdot \\sum_{s^{\\prime} \\in \\mathcal{N}} \\mathcal{P}\\left(s, a, s^{\\prime}\\right) \\cdot V^{*}\\left(s^{\\prime}\\right)\\right\\}$ for all $s \\in \\mathcal{N}$\n",
    "\n",
    "**MDP Action-Value Function Bellman Optimality Equation**\n",
    "\n",
    "$Q^{*}(s, a)=\\mathcal{R}(s, a)+\\gamma \\cdot \\sum_{s^{\\prime} \\in \\mathcal{N}} \\mathcal{P}\\left(s, a, s^{\\prime}\\right) \\cdot \\max _{a^{\\prime} \\in \\mathcal{A}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)$ for all $s \\in \\mathcal{N}, a \\in \\mathcal{A}$\n",
    "\n",
    "\n",
    "MDP Bellman Optimality Equations address the ultimate purpose of Markov Decision Processes\n",
    "\n",
    "- to identify the Optimal Value Function and the associated policy/policies\n",
    "- enabling us to solve the MDP Control problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Policy as one that dominates all others**\n",
    "\n",
    "$\\pi^{*} \\in \\Pi$ is an Optimal Policy if $V^{\\pi^{*}}(s) \\geq V^{\\pi}(s)$ for all $\\pi \\in \\Pi$ and for all states $s \\in \\mathcal{N}$\n",
    "\n",
    "**Theorem 2.0.1.** \n",
    "\n",
    "For any (discrete-time, countable-states, stationary) MDP:\n",
    "- There exists an Optimal Policy $\\pi^{*} \\in \\Pi$, i.e., there exists a Policy $\\pi^{*} \\in \\Pi$ such that $V^{\\pi^{*}}(s) \\geq V^{\\pi}(s)$ for all policies $\\pi \\in \\Pi$ and for all states $s \\in \\mathcal{N}$\n",
    "- All Optimal Policies achieve the Optimal Value Function, i.e. $V^{\\pi^{*}}(s)=V^{*}(s)$ for all $s \\in \\mathcal{N}$, for all Optimal Policies $\\pi^{*}$\n",
    "- All Optimal Policies achieve the Optimal Action-Value Function, i.e. $Q^{\\pi^{*}}(s, a)=$ $Q^{*}(s, a)$ for all $s \\in \\mathcal{N}$, for all $a \\in \\mathcal{A}$, for all Optimal Policies $\\pi^{*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 2.0.2**. \n",
    "\n",
    "For any two Optimal Policies $\\pi_{1}^{*}$ and $\\pi_{2}^{*}, V^{\\pi_{1}^{*}}(s)=V^{\\pi_{2}^{*}}(s)$ for all $s \\in \\mathcal{N}$ \n",
    "\n",
    "**Lemma 2.0.2 Proof.** \n",
    "\n",
    "Since $\\pi_{1}^{*}$ is an Optimal Policy, from the Optimal Policy definition, we have: $V^{\\pi_{1}^{*}}(s) \\geq V^{\\pi_{2}^{*}}(s)$ for all $s \\in \\mathcal{N}$. Likewise, since $\\pi_{2}^{*}$ is an Optimal Policy, from the Optimal Policy definition, we have: $V^{\\pi_{2}^{*}}(s) \\geq V^{\\pi_{1}^{2}}(s)$ for all $s \\in \\mathcal{N}$. This implies: $V^{\\pi_{1}^{*}}(s)=V^{\\pi_{2}^{*}}(s)$ for all $s \\in \\mathcal{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\pi_{D}^{*}(s)=\\underset{a \\in \\mathcal{A}}{\\arg \\max } Q^{*}(s, a) \\text { for all } s \\in \\mathcal{N}\n",
    "$$\n",
    "\n",
    "\n",
    "we show that $\\pi_{D}^{*}$ achieves the Optimal Value Functions $V^{*}$ and $Q^{*}$. \n",
    "\n",
    "Since $\\pi_{D}^{*}(s)=\\arg \\max _{a \\in \\mathcal{A}} Q^{*}(s, a)$ and $V^{*}(s)=\\max _{a \\in \\mathcal{A}} Q^{*}(s, a)$ for all $s \\in \\mathcal{N}$, we can infer for all $s \\in \\mathcal{N}$ that:\n",
    "$$\n",
    "V^{*}(s)=Q^{*}\\left(s, \\pi_{D}^{*}(s)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{gathered}\n",
    "V^{\\pi_{D}^{*}}(s)=V^{*}(s) \\text { for all } s \\in \\mathcal{N} \\\\\n",
    "Q^{\\pi_{D}^{*}}(s, a)=Q^{*}(s, a) \\text { for all } s \\in \\mathcal{N}, \\text { for all } a \\in \\mathcal{A}\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
